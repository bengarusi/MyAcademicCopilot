import os
from dotenv import load_dotenv
import litellm

#    砖  -SDK v3
from langfuse import observe


load_dotenv()

# ----- Gemini API Key -----
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    raise RuntimeError("GEMINI_API_KEY is missing in .env")

# LiteLLM 驻砖 转 驻转 砖转 住
os.environ["GEMINI_API_KEY"] = GEMINI_API_KEY

# ----- Langfuse keys -----
# 住驻拽 砖专转 转 -.env:
# LANGFUSE_SECRET_KEY=sk-lf-...
# LANGFUSE_PUBLIC_KEY=pk-lf-...
# LANGFUSE_BASE_URL=https://cloud.langfuse.com
# -SDK 拽专 转 ,  爪专 爪专 client 转.

DEFAULT_MODEL = "gemini/gemini-2.5-flash"


@observe(name="llm_generation")
def ask_llm(
    prompt: str,
    model: str = DEFAULT_MODEL,
    temperature: float = 0.2,
    max_tokens: int | None = None,
) -> str:
    """
    驻 -LLM 专 LiteLLM + 转注 -Langfuse,
    注 转 -temperature -max_tokens  砖"" 砖 住 转砖.
    """
    response = litellm.completion(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response["choices"][0]["message"]["content"]
